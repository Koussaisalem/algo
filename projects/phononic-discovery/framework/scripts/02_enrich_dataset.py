from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Sequence

import torch
from nequip.data import AtomicDataDict
from xtb.interface import Calculator, Param

REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from qcmd_hybrid_framework.manifold_utils import (  # noqa: E402
    compute_manifold_frame,
    gradient_hartree_per_bohr_to_force_ev_per_ang,
    hartree_to_ev,
)

DTYPE = torch.float64


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run GFN2-xTB enrichment for the hybrid QCMD dataset."
    )
    parser.add_argument(
        "--input-path",
        type=Path,
        default=Path("data/qm9_micro_5k.pt"),
        help="Torch file generated by 01_prepare_data.py.",
    )
    parser.add_argument(
        "--output-path",
        type=Path,
        default=Path("data/qm9_micro_5k_enriched.pt"),
        help="Destination file for the enriched samples.",
    )
    parser.add_argument(
        "--failures-path",
        type=Path,
        default=None,
        help="Optional JSON log for molecules that fail the xTB calculation.",
    )
    parser.add_argument(
        "--start-index",
        type=int,
        default=0,
        help="Start processing from this index (useful when resuming).",
    )
    parser.add_argument(
        "--max-molecules",
        type=int,
        default=None,
        help="Optional cap on the number of molecules to enrich after start-index.",
    )
    parser.add_argument(
        "--checkpoint-every",
        type=int,
        default=25,
        help="Persist intermediate results every N successful molecules.",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from an existing output file instead of overwriting it.",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Allow overwriting an existing output file when not resuming.",
    )
    return parser.parse_args()


def load_raw_dataset(path: Path) -> Sequence[Dict[str, Any]]:
    if not path.exists():
        raise FileNotFoundError(f"Input dataset not found: {path}")
    dataset = torch.load(path, map_location="cpu")
    if not isinstance(dataset, Sequence):
        raise TypeError("Expected input dataset to be a sequence of dict samples")
    return dataset


def prepare_output(output_path: Path, resume: bool, overwrite: bool) -> List[Dict[str, Any]]:
    if output_path.exists():
        if resume:
            data = torch.load(output_path, map_location="cpu")
            if not isinstance(data, list):
                raise TypeError("Existing enrichment file must be a list")
            return data
        if not overwrite:
            raise FileExistsError(
                f"{output_path} exists. Use --resume or --overwrite to proceed."
            )
    return []


def prepare_failures_log(path: Path, resume: bool) -> List[Dict[str, Any]]:
    if resume and path.exists():
        with path.open("r", encoding="utf-8") as handle:
            return json.load(handle)
    return []


def run_xtb(numbers: torch.Tensor, positions: torch.Tensor) -> tuple[float, torch.Tensor, torch.Tensor]:
    numbers_np = numbers.to(dtype=torch.int64).cpu().numpy()
    positions_np = positions.to(dtype=DTYPE).cpu().numpy()
    calculator = Calculator(Param.GFN2xTB, numbers_np, positions_np)
    result = calculator.singlepoint()
    energy_hartree = float(result.get_energy())
    gradient = torch.as_tensor(result.get_gradient(), dtype=DTYPE)
    orbitals = torch.as_tensor(result.get_orbital_coefficients(), dtype=DTYPE)
    return energy_hartree, gradient, orbitals


def enrich_sample(index: int, sample: Dict[str, Any]) -> Dict[str, Any]:
    numbers = torch.as_tensor(sample[AtomicDataDict.ATOM_TYPE_KEY], dtype=torch.int64)
    positions = torch.as_tensor(sample[AtomicDataDict.POSITIONS_KEY], dtype=DTYPE)

    if positions.ndim != 2 or positions.shape[1] != 3:
        raise ValueError("Positions must have shape (n_atoms, 3)")

    energy_hartree, gradient_hartree_bohr, orbitals = run_xtb(numbers, positions)
    forces_ev_ang = gradient_hartree_per_bohr_to_force_ev_per_ang(gradient_hartree_bohr)
    manifold_frame = compute_manifold_frame(positions, numbers)

    enriched = {
        "index": index,
        AtomicDataDict.ATOM_TYPE_KEY: numbers,
        AtomicDataDict.POSITIONS_KEY: positions,
        "energy_hartree": torch.tensor(energy_hartree, dtype=DTYPE),
        "energy_ev": hartree_to_ev(energy_hartree),
        "gradient_hartree_per_bohr": gradient_hartree_bohr,
        "forces_ev_per_angstrom": forces_ev_ang,
        "orbitals": orbitals,
        "manifold_frame": manifold_frame.as_dict(),
        "num_atoms": positions.shape[0],
    }

    return enriched


def checkpoint(output: Path, data: List[Dict[str, Any]]) -> None:
    output.parent.mkdir(parents=True, exist_ok=True)
    torch.save(data, output)


def checkpoint_failures(path: Path, failures: List[Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(failures, handle, indent=2)


def main() -> None:
    args = parse_args()

    raw_dataset = load_raw_dataset(args.input_path)
    enriched_samples = prepare_output(args.output_path, args.resume, args.overwrite)

    failures_path = args.failures_path
    if failures_path is None:
        failures_path = args.output_path.with_suffix(".failures.json")
    failures = prepare_failures_log(failures_path, args.resume)

    start_index = max(args.start_index, len(enriched_samples)) if args.resume else args.start_index
    if start_index >= len(raw_dataset):
        raise IndexError(
            f"start-index {start_index} exceeds dataset size {len(raw_dataset)}"
        )

    limit = len(raw_dataset) if args.max_molecules is None else min(
        len(raw_dataset), start_index + args.max_molecules
    )

    print(
        f"Enrichment starting at index {start_index} / {len(raw_dataset)} (target {limit})."
    )

    since_checkpoint = 0

    for idx in range(start_index, limit):
        sample = raw_dataset[idx]
        try:
            enriched = enrich_sample(idx, sample)
        except Exception as exc:  # noqa: BLE001
            failure_record = {"index": idx, "error": str(exc)}
            failures.append(failure_record)
            print(f"[WARN] xTB failed for molecule {idx}: {exc}")
            continue

        enriched_samples.append(enriched)
        since_checkpoint += 1

        energy_ev = enriched["energy_ev"].item()
        print(f"[OK] Molecule {idx} enriched (energy = {energy_ev:.6f} eV)")

        if since_checkpoint >= args.checkpoint_every:
            checkpoint(args.output_path, enriched_samples)
            checkpoint_failures(failures_path, failures)
            since_checkpoint = 0

    checkpoint(args.output_path, enriched_samples)
    checkpoint_failures(failures_path, failures)

    print(
        f"Enrichment complete. Stored {len(enriched_samples)} molecules. "
        f"Failures logged: {len(failures)} -> {failures_path}"
    )


if __name__ == "__main__":
    main()
